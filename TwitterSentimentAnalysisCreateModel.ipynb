{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and restart the Kernel before running the other cells\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the date to today\n",
    "\n",
    "# Parse JSON and display the number of tweets\n",
    "df_tweets = spark.read.json(\"hdfs:///user/nobody/tweet-lake/raw/2018/08/14/*\")\n",
    "df_tweets.createOrReplaceTempView(\"tweets\")\n",
    "df_tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find tweets containing emojis and use the emojis to define if they are positive or negative tweets\n",
    "df_positive_tweets = spark.sql(\"SELECT text FROM tweets WHERE (text LIKE '%\\U0001F60D%' OR text LIKE '%\\U0001F60A%' OR text LIKE '%\\U0001F604%' OR text LIKE '%\\U0001F603%' OR text LIKE '%\\U0001F600%' OR text LIKE '%\\U0001F606%') AND text NOT LIKE '%\\U0001F62D%' AND text NOT LIKE '%\\U0001F612%' AND text NOT LIKE '%\\U0001F629%' AND text NOT LIKE '%\\U0001F61E%' AND text NOT LIKE '%\\U0001F62A%'\")\n",
    "df_positive_tweets.createOrReplaceTempView(\"positive_tweets\")\n",
    "df_negative_tweets = spark.sql(\"SELECT text FROM tweets WHERE (text LIKE '%\\U0001F62D%' OR text LIKE '%\\U0001F612%' OR text LIKE '%\\U0001F629%' OR text LIKE '%\\U0001F61E%' OR text LIKE '%\\U0001F62A%') AND text NOT LIKE '%\\U0001F60D%' AND text NOT LIKE '%\\U0001F60A%' AND text NOT LIKE '%\\U0001F604%' AND text NOT LIKE '%\\U0001F603%' AND text NOT LIKE '%\\U0001F600%' AND text NOT LIKE '%\\U0001F606%'\")\n",
    "df_negative_tweets.createOrReplaceTempView(\"negative_tweets\")\n",
    "df_sentiments = spark.sql(\"(SELECT text, CAST(1 AS DOUBLE) AS sentiment FROM positive_tweets) UNION ALL (SELECT text, CAST(0 AS DOUBLE) AS sentiment FROM negative_tweets) ORDER BY RAND()\")\n",
    "df_sentiments.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean (a little bit) the tweets\n",
    "# Taken from https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-2-333514854913\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner_updated(row):\n",
    "    text = row.text\n",
    "    stripped = re.sub(combined_pat, '', text)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    simple_spaced = re.sub(' +',' ',letters_only)\n",
    "    return simple_spaced, row.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the tweets and only keep the ones that have more than 50 characters after cleanup\n",
    "rdd_clean = df_sentiments.rdd.map(tweet_cleaner_updated)\n",
    "df_clean = rdd_clean.toDF([\"text\",\"sentiment\"])\n",
    "df_clean.createOrReplaceTempView(\"clean\")\n",
    "df_final = spark.sql(\"SELECT * FROM clean WHERE LENGTH(text) > 50\")\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset for training, validation and testing purpose\n",
    "(train_set, val_set, test_set) = df_final.randomSplit([0.90, 0.05, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and evaluate the model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=20)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(val_df)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in HDFS\n",
    "lrModel.save(\"hdfs:///user/client/tweets_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
