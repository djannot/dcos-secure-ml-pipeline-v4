{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import org.apache.spark.ml.classification.LogisticRegressionModel\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "// Import the sentiment analysis model from HDFS\n",
    "val modelUri = \"hdfs:///user/client/tweets_model\"\n",
    "val model = LogisticRegressionModel.load(modelUri)\n",
    "\n",
    "// Get tweets from Kafka\n",
    "val ssc = new StreamingContext(spark.sparkContext, Seconds(2))\n",
    "val topic = \"tweets\"\n",
    "val brokers = \"kafka-0-broker.kafka.autoip.dcos.thisdcos.directory:1025\"\n",
    "val props = Map[String, Object](\n",
    "  \"bootstrap.servers\" -> brokers,\n",
    "  \"key.deserializer\" -> classOf[StringDeserializer],\n",
    "  \"value.deserializer\" -> classOf[StringDeserializer],\n",
    "  \"group.id\" -> \"consumer1\",\n",
    "  \"auto.offset.reset\" -> \"earliest\",\n",
    "  \"enable.auto.commit\" -> (false: java.lang.Boolean),\n",
    "  \"sasl.kerberos.service.name\" -> \"kafka\",\n",
    "  \"security.protocol\" -> \"SASL_SSL\",\n",
    "  \"sasl.mechanism\" -> \"GSSAPI\",\n",
    "  \"ssl.truststore.location\" -> \"/mnt/mesos/sandbox/trust-ca.jks\",\n",
    "  \"ssl.truststore.password\" -> \"changeit\"\n",
    ")\n",
    "val stream = KafkaUtils.createDirectStream[String, String](\n",
    "  ssc,\n",
    "  LocationStrategies.PreferConsistent,\n",
    "  ConsumerStrategies.Subscribe[String, String](Array(topic), props))\n",
    "val lines = stream.map(_.value)\n",
    "\n",
    "// Define a function to clean (a little bit) the tweets\n",
    "val combinedPattern = \"\"\"@[A-Za-z0-9_]+|https?://[^ ]+\"\"\".r\n",
    "val wwwPattern = \"\"\"www.[^ ]+\"\"\".r\n",
    "val negationsDict: Map[String, String] = Map((\"isn't\",\"is not\"),(\"aren't\",\"are not\"),(\"wasn't\",\"was not\"),(\"weren't\",\"were not\"),(\"haven't\",\"have not\"),(\"hasn't\",\"has not\"),(\"hadn't\",\"had not\"),(\"won't\",\"will not\"),(\"wouldn't\",\"would not\"),(\"don't\",\"do not\"),(\"doesn't\",\"does not\"),(\"didn't\",\"did not\"),(\"can't\",\"can not\"),(\"couldn't\",\"could not\"),(\"shouldn't\",\"should not\"),(\"mightn't\",\"might not\"),(\"mustn't\",\"must not\"))\n",
    "def cleanTweets(s: String): String = {\n",
    "  var stripped = combinedPattern.replaceAllIn(s, \"\")\n",
    "  stripped = wwwPattern.replaceAllIn(stripped, \"\")\n",
    "  val lowerCase = stripped.toLowerCase()\n",
    "  var negHandled = lowerCase\n",
    "  for ((k,v) <- negationsDict) {\n",
    "    negHandled = negHandled.replaceAll(k, v)\n",
    "  }\n",
    "  val lettersOnly = \"\"\"[^a-zA-Z]\"\"\".r.replaceAllIn(negHandled, \" \")\n",
    "  val simpleSpaced = \"\"\" +\"\"\".r.replaceAllIn(lettersOnly, \" \")\n",
    "  return simpleSpaced\n",
    "}\n",
    "\n",
    "lines.foreachRDD { rdd: RDD[String] =>\n",
    "  // Parse JSON\n",
    "  val rawDf = spark.read.json(rdd)\n",
    "  if(rawDf.count() > 0) {\n",
    "    // Clean the tweers\n",
    "    val cleanTweetsUdf = udf(cleanTweets _)\n",
    "    var cleanDf = rawDf.select($\"text\", cleanTweetsUdf(rawDf(\"text\")))\n",
    "    cleanDf = cleanDf.withColumnRenamed(\"text\", \"tweet\").withColumnRenamed(\"UDF(text)\", \"text\")\n",
    "    cleanDf.createOrReplaceTempView(\"clean\")\n",
    "    // Only keep the tweets that have more than 50 characters after cleanup\n",
    "    val finalDf = spark.sql(\"SELECT * FROM clean WHERE LENGTH(text) > 50\")\n",
    "    // Prepare the data for the model\n",
    "    val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "    val hashingTF = new HashingTF().setInputCol(\"words\").setOutputCol(\"tf\").setNumFeatures(65536)\n",
    "    val idf = new IDF().setMinDocFreq(5).setInputCol(\"tf\").setOutputCol(\"features\")\n",
    "    val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, idf))\n",
    "    val pipelineFit = pipeline.fit(finalDf)\n",
    "    val preparedDf = pipelineFit.transform(finalDf)\n",
    "    // Predict sentiment using the model\n",
    "    val predictionsDf = model.transform(preparedDf)\n",
    "    // Display the predictions\n",
    "    predictionsDf.select(\"tweet\", \"prediction\").show(50, false)\n",
    "  }\n",
    "}\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
